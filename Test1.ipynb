{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un premier reseau à trois couches et 3 paramètres\n",
    "Résolution du problème inverse d'intégrale multiple\n",
    "Pour un réseau de neurones Unfolding\n",
    "Chaque couche n'a que trois paramètres.\n",
    "\n",
    "On rappelle le modèle.\n",
    "We set an initial value $x_0$ and\n",
    "we introduce the following $m$-layer neural network\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\label{def:modelG}\n",
    "\t\\begin{cases}\n",
    "\t\\textbf{Initialization:} \\\\\n",
    "\t\\quad b_0 = T^*y^\\delta ,\\\\\n",
    "\t\\textbf{Layer $n\\in \\{1,\\ldots,m\\}$:} \\\\\n",
    "      \\quad x_n %=  Q_n(x_0,x_{n-1}) \n",
    "          = R_n(W_n x_{n-1} + W_{n,0}b_0)\\;,\n",
    "    \\end{cases}\t\t  \n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{align}\n",
    "&R_n = \\text{prox}_{\\lambda_n \\mu_n g}\\\\\n",
    "&W_n =  1 - \\lambda_n T^*T - \\lambda_n \\alpha_n D^*D \\\\\n",
    "&W_{n,0} = \\lambda_n 1.\n",
    "\\end{align}\n",
    "\n",
    "Dansd un premier temps on apprend seulement le paramètre $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "1 - first create the panda frame (or numpy)\n",
    "\n",
    "2 - transform into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical data\n",
    "l = 200\n",
    "tau = 100\n",
    "dep = 2\n",
    "# Numerical data\n",
    "nx = 200\n",
    "dx = l/(nx+1)\n",
    "nt = 200\n",
    "dt = tau/nt\n",
    "T_operator = 1/100*dx*np.tri(nt, nx, 0, dtype=int)\n",
    "# Data sample\n",
    "nsamp = 150\n",
    "x_dagger = np.zeros((nx,nsamp))\n",
    "y = np.zeros((nt,nsamp))\n",
    "x_grid = np.linspace(0,l,nx)\n",
    "#\n",
    "x_sample = np.zeros((nx,nsamp))\n",
    "#\n",
    "for i in range(0,nsamp):\n",
    "    mu = l/2\n",
    "    sigma = 0.1\n",
    "    x_dagger[:,i] = (sigma*np.sqrt(2*np.pi))**-1*np.exp(-(x_grid-mu)**2/2*sigma**2)\n",
    "    y[:,i] = T_operator.dot(x_dagger[:,i]) \n",
    "    xi = np.random.uniform(-0.05,0.05,nt)\n",
    "    y[:,i] += 0.05*xi*np.linalg.norm(y[:,i])/np.linalg.norm(xi)\n",
    "    x_sample[:,i] = np.transpose(T_operator).dot(y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "train_X = x_sample[:,:100]\n",
    "test_X = x_sample[:,100:]\n",
    "train_y = x_dagger[:,:100]\n",
    "test_y = x_dagger[:,100:]\n",
    "# Convert to pytorch tensor\n",
    "X_train = torch.from_numpy(train_X)\n",
    "X_test = torch.from_numpy(test_X)\n",
    "y_train = torch.from_numpy(train_y)\n",
    "y_test = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9865, 0.9872, 0.9954,  ..., 0.9982, 0.9896, 0.9951],\n",
      "        [0.9861, 0.9871, 0.9959,  ..., 0.9978, 0.9896, 0.9952],\n",
      "        [0.9863, 0.9869, 0.9954,  ..., 0.9982, 0.9899, 0.9952],\n",
      "        ...,\n",
      "        [0.0296, 0.0288, 0.0310,  ..., 0.0290, 0.0288, 0.0298],\n",
      "        [0.0197, 0.0188, 0.0206,  ..., 0.0198, 0.0194, 0.0199],\n",
      "        [0.0093, 0.0096, 0.0102,  ..., 0.0096, 0.0098, 0.0097]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation operator\n",
    "D_operator = np.diag(np.ones(nx-1),1)+ np.diag(np.ones(nx-1),-1)-2*np.eye(nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x, T, D, nx):\n",
    "        tensor_grad = torch.eye(nx)\\\n",
    "        - torch.from_numpy(np.transpose(T).dot(T)) \\\n",
    "        - self.alpha*torch.from_numpy(np.transpose(D).dot(D))\n",
    "        y_pred = torch.matmul(tensor_grad,x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.3007], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.7484])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[:,1].size())\n",
    "y_pred = model.forward(X_train[:,1],T_operator,D_operator,nx)\n",
    "y_pred.size()\n",
    "y_pred.norm().backward()\n",
    "model.alpha.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1 - Set the loss function\n",
    "\n",
    "2 - Set the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0518, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "criterion(y_train[:,99], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 loss: 2.05564942 weight: -1.29985523\n",
      "epoch:  2 loss: 2.05530897 weight: -1.29979897\n",
      "epoch:  3 loss: 2.05223881 weight: -1.29974294\n",
      "epoch:  4 loss: 2.05208899 weight: -1.29968667\n",
      "epoch:  5 loss: 2.04390520 weight: -1.29963195\n",
      "epoch:  6 loss: 2.05387928 weight: -1.29957569\n",
      "epoch:  7 loss: 2.05328127 weight: -1.29951954\n",
      "epoch:  8 loss: 2.04785301 weight: -1.29946375\n",
      "epoch:  9 loss: 2.06287786 weight: -1.29940629\n",
      "epoch: 10 loss: 2.04563425 weight: -1.29935098\n",
      "epoch: 11 loss: 2.05429536 weight: -1.29929471\n",
      "epoch: 12 loss: 2.04740326 weight: -1.29923987\n",
      "epoch: 13 loss: 2.05178695 weight: -1.29918444\n",
      "epoch: 14 loss: 2.05165940 weight: -1.29912806\n",
      "epoch: 15 loss: 2.05497582 weight: -1.29907191\n",
      "epoch: 16 loss: 2.04806677 weight: -1.29901636\n",
      "epoch: 17 loss: 2.06311915 weight: -1.29895937\n",
      "epoch: 18 loss: 2.04386546 weight: -1.29890478\n",
      "epoch: 19 loss: 2.05435282 weight: -1.29884863\n",
      "epoch: 20 loss: 2.05496013 weight: -1.29879248\n",
      "epoch: 21 loss: 2.04203310 weight: -1.29873788\n",
      "epoch: 22 loss: 2.05466152 weight: -1.29868186\n",
      "epoch: 23 loss: 2.04893340 weight: -1.29862618\n",
      "epoch: 24 loss: 2.05259741 weight: -1.29857004\n",
      "epoch: 25 loss: 2.05217006 weight: -1.29851413\n",
      "epoch: 26 loss: 2.04377124 weight: -1.29845893\n",
      "epoch: 27 loss: 2.05367011 weight: -1.29840302\n",
      "epoch: 28 loss: 2.04949016 weight: -1.29834759\n",
      "epoch: 29 loss: 2.04447126 weight: -1.29829288\n",
      "epoch: 30 loss: 2.04977737 weight: -1.29823697\n",
      "epoch: 31 loss: 2.05089165 weight: -1.29818106\n",
      "epoch: 32 loss: 2.05106379 weight: -1.29812527\n",
      "epoch: 33 loss: 2.04556346 weight: -1.29807007\n",
      "epoch: 34 loss: 2.05274055 weight: -1.29801440\n",
      "epoch: 35 loss: 2.05568112 weight: -1.29795825\n",
      "epoch: 36 loss: 2.05549726 weight: -1.29790199\n",
      "epoch: 37 loss: 2.05087291 weight: -1.29784608\n",
      "epoch: 38 loss: 2.06305245 weight: -1.29778910\n",
      "epoch: 39 loss: 2.05126552 weight: -1.29773366\n",
      "epoch: 40 loss: 2.04197537 weight: -1.29767919\n",
      "epoch: 41 loss: 2.04308201 weight: -1.29762423\n",
      "epoch: 42 loss: 2.05944836 weight: -1.29756737\n",
      "epoch: 43 loss: 2.04442858 weight: -1.29751265\n",
      "epoch: 44 loss: 2.05200160 weight: -1.29745710\n",
      "epoch: 45 loss: 2.04371293 weight: -1.29740191\n",
      "epoch: 46 loss: 2.05038252 weight: -1.29734635\n",
      "epoch: 47 loss: 2.06302399 weight: -1.29728937\n",
      "epoch: 48 loss: 2.05003755 weight: -1.29723430\n",
      "epoch: 49 loss: 2.05655014 weight: -1.29717839\n",
      "epoch: 50 loss: 2.04194512 weight: -1.29712391\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    k = rand.randint(0,99)\n",
    "    y_pred = model.forward(X_train[:,k],T_operator,D_operator,nx)\n",
    "    loss = criterion(y_pred, y_train[:,1])\n",
    "    losses.append(loss)\n",
    "    #\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'epoch: {i:2} loss: {loss.item():10.8f} weight: {model.alpha.item() :10.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layers feedforward network, try #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.4724090099334717\n",
      "199 0.05619945377111435\n",
      "299 0.0021141788456588984\n",
      "399 9.78464086074382e-05\n",
      "499 5.4044739954406396e-06\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
