{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "on entraine le réseau de neurones pour les hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# local\n",
    "from MyResNet.myfunc import Physics\n",
    "from MyResNet.myfunc import MyMatmul\n",
    "from MyResNet.model import MyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramètres physiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'nx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-de45ec8a849d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtT\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mphys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComputeAdjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'nx'"
     ]
    }
   ],
   "source": [
    "phys = Physics()\n",
    "tT   = phys.ComputeAdjoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLoader(folder,nsample):\n",
    "    \"\"\"\n",
    "    According to the mode, creates the appropriate loader \n",
    "    for the training and validation sets.\n",
    "    \"\"\"\n",
    "    dfx     = pd.read_csv(folder+'/'+'data_lisse.csv', sep=',',header=None)\n",
    "    dfy     = pd.read_csv(folder+'/'+'data_blurred.csv', sep=',',header=None)\n",
    "    _,nx    = dfx.shape\n",
    "    #\n",
    "    x_tensor= torch.FloatTensor(dfx.values[:nsample]).view(-1,1,nx)\n",
    "    y_tensor= torch.FloatTensor(dfy.values[:nsample]).view(-1,1,nx)\n",
    "    #\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    l = len(dataset)\n",
    "    m = 2*l//3\n",
    "    train_dataset, val_dataset = random_split(dataset, [m, l-m])\n",
    "    #\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder  = './MyResNet/Dataset'\n",
    "nsample = 50\n",
    "t, v    = CreateLoader(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyModel(tensor_list,mass,U,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,nb_epochs):\n",
    "    \"\"\"\n",
    "    Trains iRestNet.\n",
    "    \"\"\"      \n",
    "    # to store results\n",
    "    loss_train   =  np.zeros(nb_epochs)\n",
    "    loss_val     =  np.zeros(nb_epochs)\n",
    "    loss_min_val =  float('Inf')\n",
    "    # defines the optimizer\n",
    "    lr_i        = 0.01\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()),lr=lr_i)\n",
    "\n",
    "    #==========================================================================================================\n",
    "    # trains for several epochs\n",
    "    for epoch in range(0,nb_epochs): \n",
    "        # sets training mode\n",
    "        model.train()\n",
    "        # modifies learning rate\n",
    "        if epoch>0:\n",
    "            lr_i      = lr_i*0.9 \n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=lr_i)\n",
    "        # TRAINING\n",
    "        # goes through all minibatches\n",
    "        for i,minibatch in enumerate(t):\n",
    "            [x_true, x_blurred] = minibatch    # get the minibatch\n",
    "            x_true    = Variable(x_true,requires_grad=False)\n",
    "            x_init    = Variable(x_blurred,requires_grad=False)\n",
    "            # ATTENTION : on ne calcule pas le gradient en fonction de la deuxième sortie (à réfléchir)\n",
    "            Ttx_init  = Tt(x_init).detach()     # do not compute gradient\n",
    "            x_pred    = model(x_init,Ttx_init) \n",
    "                    \n",
    "            # Computes and prints loss\n",
    "            loss                = loss_fn(x_pred, x_true)\n",
    "            loss_train[epoch] += torch.Tensor.item(loss)\n",
    "                    \n",
    "            # sets the gradients to zero, performs a backward pass, and updates the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # normalisation\n",
    "        loss_train[epoch] = loss_train[epoch]/i\n",
    "        #\n",
    "        #\n",
    "        # VALIDATION AND STATS\n",
    "        if epoch%1==0:\n",
    "            with torch.no_grad():\n",
    "            # saves signal and model state  \n",
    "            # utils.save_image(x_pred.data,os.path.join(\n",
    "            #            folder,'training',str(epoch)+'_restored_images.png'),normalize=True)\n",
    "            # torch.save(self.last_layer.state_dict(),os.path.join(folder,'trained_post-processing.pt'))\n",
    "            # torch.save(self.model.state_dict(),os.path.join(folder,'trained_model.pt'))\n",
    "\n",
    "            # tests on validation set\n",
    "                model.eval()      # evaluation mode\n",
    "                for i,minibatch in enumerate(v):\n",
    "                    [x_true, x_blurred] = minibatch            # gets the minibatch\n",
    "                    x_true    = Variable(x_true,requires_grad=False)\n",
    "                    x_init    = Variable(x_blurred,requires_grad=False)\n",
    "                    Tt_x_init = Tt(x_init).detach()        # does not compute gradient\n",
    "                    x_pred = model(x_init,Tt_x_init) \n",
    "                    \n",
    "                    # computes loss on validation set\n",
    "                    loss_val[epoch] += torch.Tensor.item(loss_fn(x_pred, x_true))\n",
    "                # normalisation\n",
    "                loss_val[epoch] = loss_val[epoch]/i\n",
    "            # print stat\n",
    "            print(\"epoch : \", epoch,\" ----- \",\"validation : \",loss_val[epoch])\n",
    "\n",
    "\n",
    "               \n",
    "               \n",
    "            \n",
    "    #==========================================================================================================\n",
    "    # training is finished\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('Training is done.')\n",
    "    print('-----------------------------------------------------------------')\n",
    "\n",
    "    return loss_train, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t, loss_v = train(mymodel,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_t,label = 'train')\n",
    "plt.plot(loss_v,label = 'val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = []\n",
    "x_ref  = []\n",
    "x_b    = []\n",
    "for i,minibatch in enumerate(v):\n",
    "    [x_true, x_blurred] = minibatch    # get the minibatch\n",
    "    x_true    = Variable(x_true,requires_grad=False)\n",
    "    x_init    = Variable(x_blurred,requires_grad=False)\n",
    "    # ATTENTION : on ne calcule pas le gradient en fonction de la deuxième sortie (à réfléchir)\n",
    "    Ttx_init  = Tt(x_init).detach()     # do not compute gradient\n",
    "    x_pred.append(mymodel(x_init,Ttx_init).detach())\n",
    "    x_ref.append(x_true.detach())\n",
    "    x_b.append(x_init.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_pred = mymodel(x_init,Ttx_init,save_gamma_mu_lambda='yes').detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_b[1].squeeze().numpy())\n",
    "plt.plot(x_b[2].squeeze().numpy())\n",
    "plt.plot(x_b[11].squeeze().numpy())\n",
    "plt.plot(x_b[12].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Tt(x_b[1]).squeeze().numpy())\n",
    "plt.plot(Tt(x_b[2]).squeeze().numpy())\n",
    "plt.plot(Tt(x_b[11]).squeeze().numpy())\n",
    "plt.plot(Tt(x_b[12]).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt(x_b[1]).squeeze()[0]\n",
    "Tt(x_b[2]).squeeze()[0]\n",
    "Tt(x_b[3]).squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[11].squeeze().numpy())\n",
    "plt.plot(x_ref[11].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[10].squeeze().numpy())\n",
    "plt.plot(x_ref[10].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[15].squeeze().numpy())\n",
    "plt.plot(x_ref[15].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[2].squeeze().numpy())\n",
    "plt.plot(x_ref[2].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[1].squeeze().numpy())\n",
    "plt.plot(x_pred[2].squeeze().numpy())\n",
    "plt.plot(x_pred[3].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D  = nx**2*(2*np.diag(np.ones(nx)) - np.diag(np.ones(nx-1),-1) - np.diag(np.ones(nx-1),1))# matrice de dérivation\n",
    "Top  = 1/nx*np.tri(nx, nx, 0, dtype=int).T # matrice de convolution\n",
    "plt.imshow(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "D1 = sqrtm(D)\n",
    "plt.imshow(Top.dot(D1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(D1.dot(Top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00000000001\n",
    "rho = 1\n",
    "xk  = np.zeros(nx)\n",
    "Ttx = Tt(x_b[5]).squeeze().numpy()\n",
    "for k in range(50):\n",
    "    xk -=rho*((Top.T.dot(Top)).dot(xk)-Ttx+alpha*(D.T).dot(D).dot(xk))\n",
    "              \n",
    "plt.plot(xk)\n",
    "plt.plot(x_ref[5].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
