{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un premier reseau à trois couches et 3 paramètres\n",
    "Résolution du problème inverse d'intégrale multiple\n",
    "Pour un réseau de neurones Unfolding\n",
    "Chaque couche n'a que trois paramètres.\n",
    "\n",
    "On rappelle le modèle.\n",
    "We set an initial value $x_0$ and\n",
    "we introduce the following $m$-layer neural network\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\label{def:modelG}\n",
    "\t\\begin{cases}\n",
    "\t\\textbf{Initialization:} \\\\\n",
    "\t\\quad b_0 = T^*y^\\delta ,\\\\\n",
    "\t\\textbf{Layer $n\\in \\{1,\\ldots,m\\}$:} \\\\\n",
    "      \\quad x_n %=  Q_n(x_0,x_{n-1}) \n",
    "          = R_n(W_n x_{n-1} + W_{n,0}b_0)\\;,\n",
    "    \\end{cases}\t\t  \n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{align}\n",
    "&R_n = \\text{prox}_{\\lambda_n \\mu_n g}\\\\\n",
    "&W_n =  1 - \\lambda_n T^*T - \\lambda_n \\alpha_n D^*D \\\\\n",
    "&W_{n,0} = \\lambda_n 1.\n",
    "\\end{align}\n",
    "\n",
    "Dansd un premier temps on apprend seulement le paramètre $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "1 - first create the panda frame (or numpy)\n",
    "\n",
    "2 - transform into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical data\n",
    "l = 200\n",
    "tau = 100\n",
    "dep = 2\n",
    "# Numerical data\n",
    "nx = 200\n",
    "dx = l/(nx+1)\n",
    "nt = 200\n",
    "dt = tau/nt\n",
    "T_operator = 1/100*dx*np.tri(nt, nx, 0, dtype=int)\n",
    "# Data sample\n",
    "nsamp = 150\n",
    "x_dagger = np.zeros((nx,nsamp))\n",
    "y = np.zeros((nt,nsamp))\n",
    "x_grid = np.linspace(0,l,nx)\n",
    "#\n",
    "x_sample = np.zeros((nx,nsamp))\n",
    "#\n",
    "for i in range(0,nsamp):\n",
    "    mu = l/2\n",
    "    sigma = 0.1\n",
    "    x_dagger[:,i] = (sigma*np.sqrt(2*np.pi))**-1*np.exp(-(x_grid-mu)**2/2*sigma**2)\n",
    "    y[:,i] = T_operator.dot(x_dagger[:,i]) \n",
    "    xi = np.random.uniform(-0.05,0.05,nt)\n",
    "    y[:,i] += 0.05*xi*np.linalg.norm(y[:,i])/np.linalg.norm(xi)\n",
    "    x_sample[:,i] = np.transpose(T_operator).dot(y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "train_X = x_sample[:,:100]\n",
    "test_X = x_sample[:,100:]\n",
    "train_y = x_dagger[:,:100]\n",
    "test_y = x_dagger[:,100:]\n",
    "# Convert to pytorch tensor\n",
    "X_train = torch.from_numpy(train_X)\n",
    "X_test = torch.from_numpy(test_X)\n",
    "y_train = torch.from_numpy(train_y)\n",
    "y_test = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9967, 0.9903, 0.9875,  ..., 0.9927, 0.9853, 0.9899],\n",
      "        [0.9966, 0.9902, 0.9875,  ..., 0.9930, 0.9859, 0.9904],\n",
      "        [0.9967, 0.9902, 0.9879,  ..., 0.9935, 0.9862, 0.9907],\n",
      "        ...,\n",
      "        [0.0294, 0.0303, 0.0293,  ..., 0.0304, 0.0293, 0.0292],\n",
      "        [0.0193, 0.0202, 0.0197,  ..., 0.0200, 0.0194, 0.0198],\n",
      "        [0.0097, 0.0104, 0.0104,  ..., 0.0103, 0.0096, 0.0100]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation operator\n",
    "D_operator = np.diag(np.ones(nx-1),1)+ np.diag(np.ones(nx-1),-1)-2*np.eye(nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x, T, D, nx):\n",
    "        tensor_grad = torch.eye(nx)\\\n",
    "        - torch.from_numpy(np.transpose(T).dot(T)) \\\n",
    "        - self.alpha*torch.from_numpy(np.transpose(D).dot(D))\n",
    "        y_pred = torch.matmul(tensor_grad,x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.8483], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0379])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[:,1].size())\n",
    "y_pred = model.forward(X_train[:,1],T_operator,D_operator,nx)\n",
    "y_pred.size()\n",
    "y_pred.norm().backward()\n",
    "model.alpha.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1 - Set the loss function\n",
    "\n",
    "2 - Set the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0873, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "criterion(y_train[:,99], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 loss: 2.09128038 weight: -1.84714091\n",
      "epoch:  2 loss: 2.08814610 weight: -1.84705842\n",
      "epoch:  3 loss: 2.08564029 weight: -1.84697604\n",
      "epoch:  4 loss: 2.09214861 weight: -1.84689319\n",
      "epoch:  5 loss: 2.08217198 weight: -1.84681177\n",
      "epoch:  6 loss: 2.08271867 weight: -1.84673047\n",
      "epoch:  7 loss: 2.09671086 weight: -1.84664583\n",
      "epoch:  8 loss: 2.09066849 weight: -1.84656262\n",
      "epoch:  9 loss: 2.08422126 weight: -1.84648144\n",
      "epoch: 10 loss: 2.09002197 weight: -1.84639812\n",
      "epoch: 11 loss: 2.08902535 weight: -1.84631550\n",
      "epoch: 12 loss: 2.09236944 weight: -1.84623289\n",
      "epoch: 13 loss: 2.08974356 weight: -1.84615028\n",
      "epoch: 14 loss: 2.09497896 weight: -1.84606719\n",
      "epoch: 15 loss: 2.08686843 weight: -1.84598446\n",
      "epoch: 16 loss: 2.08983527 weight: -1.84590185\n",
      "epoch: 17 loss: 2.09000803 weight: -1.84581912\n",
      "epoch: 18 loss: 2.08888268 weight: -1.84573674\n",
      "epoch: 19 loss: 2.08684110 weight: -1.84565401\n",
      "epoch: 20 loss: 2.09111209 weight: -1.84557104\n",
      "epoch: 21 loss: 2.08383485 weight: -1.84548938\n",
      "epoch: 22 loss: 2.08891653 weight: -1.84540677\n",
      "epoch: 23 loss: 2.08546942 weight: -1.84532511\n",
      "epoch: 24 loss: 2.08742947 weight: -1.84524286\n",
      "epoch: 25 loss: 2.08649850 weight: -1.84516084\n",
      "epoch: 26 loss: 2.08976728 weight: -1.84507835\n",
      "epoch: 27 loss: 2.08764167 weight: -1.84499586\n",
      "epoch: 28 loss: 2.08698275 weight: -1.84491348\n",
      "epoch: 29 loss: 2.08886897 weight: -1.84483087\n",
      "epoch: 30 loss: 2.09388345 weight: -1.84474754\n",
      "epoch: 31 loss: 2.08909921 weight: -1.84466434\n",
      "epoch: 32 loss: 2.08029843 weight: -1.84458339\n",
      "epoch: 33 loss: 2.09693619 weight: -1.84449959\n",
      "epoch: 34 loss: 2.08483542 weight: -1.84441805\n",
      "epoch: 35 loss: 2.09001214 weight: -1.84433532\n",
      "epoch: 36 loss: 2.08495207 weight: -1.84425390\n",
      "epoch: 37 loss: 2.08805717 weight: -1.84417176\n",
      "epoch: 38 loss: 2.09690172 weight: -1.84408808\n",
      "epoch: 39 loss: 2.08640385 weight: -1.84400618\n",
      "epoch: 40 loss: 2.08492528 weight: -1.84392476\n",
      "epoch: 41 loss: 2.09437456 weight: -1.84384120\n",
      "epoch: 42 loss: 2.08517092 weight: -1.84375942\n",
      "epoch: 43 loss: 2.08637692 weight: -1.84367752\n",
      "epoch: 44 loss: 2.08021853 weight: -1.84359658\n",
      "epoch: 45 loss: 2.09226835 weight: -1.84351289\n",
      "epoch: 46 loss: 2.08949942 weight: -1.84343040\n",
      "epoch: 47 loss: 2.09598992 weight: -1.84334636\n",
      "epoch: 48 loss: 2.08276598 weight: -1.84326482\n",
      "epoch: 49 loss: 2.09086635 weight: -1.84318209\n",
      "epoch: 50 loss: 2.08271602 weight: -1.84310138\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    k = rand.randint(0,99)\n",
    "    y_pred = model.forward(X_train[:,k],T_operator,D_operator,nx)\n",
    "    loss = criterion(y_pred, y_train[:,1])\n",
    "    losses.append(loss)\n",
    "    #\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'epoch: {i:2} loss: {loss.item():10.8f} weight: {model.alpha.item() :10.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layers feedforward network, try #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.7188020944595337\n",
      "199 0.0202685184776783\n",
      "299 0.0005007755826227367\n",
      "399 1.7126203601947054e-05\n",
      "499 6.872847961858497e-07\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
