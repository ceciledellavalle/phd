{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Operator - test\n",
    "\n",
    "We compute with tensor the proximal operator associated to hyperslab constraint\n",
    "in order to include it as an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cdellava/Documents/phd/MyResNet\n",
      "/usr/lib/python36.zip\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/cecile/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/cecile/.local/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/cecile/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/cdellava/Documents/phd/MyResNet')\n",
    "\n",
    "for d in sys.path:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "# from IPsolver import cardan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cardan(torch.autograd.Function):  \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,gamma_mu,xtilde,u,im_range,mode_training=True):\n",
    "        \"\"\"\n",
    "\t    Finds the solution of the cubic equation involved in the computation of the proximity operator of the \n",
    "        logarithmic barrier of the hyperslab constraints (xmin< u^Tx <xmax) using the Cardano formula: x^3+ax^2+bx+c=0 \n",
    "        is rewritten as x^3+px+q=0. Selects the solution x such that x-a/3 is real and belongs to ]xmin,xmax[.\n",
    "        Parameters\n",
    "        ----------\n",
    "           gamma_mu (torch.FloatTensor): product of the barrier parameter and the stepsize, size n\n",
    "           xtilde (torch.FloatTensor): point at which the proximity operator is applied, size n\n",
    "           u_ker (torch.FloatTensor) : kernel conv u^T x_tilde, size n\n",
    "           im_range (list): minimal and maximal pixel values\n",
    "           device (string) : \n",
    "           mode_training (bool): indicates if the model is in training (True) or testing (False) (default is True)\n",
    "        Returns\n",
    "        -------\n",
    "           sol (torch.FloatTensor): proximity operator of gamma_mu*barrier at xtilde, size n \n",
    "        \"\"\"\n",
    "        # Device CPU/GPU\n",
    "        device=\"cpu\"\n",
    "        if device == \"cuda\":\n",
    "            dtype = torch.cuda.FloatTensor\n",
    "        else :\n",
    "            dtype = torch.FloatTensor\n",
    "        #initialize variables\n",
    "        size              = xtilde.size()\n",
    "        x1,x2,x3          = torch.zeros(1).type(dtype),torch.zeros(1).type(dtype),torch.zeros(1).type(dtype)   \n",
    "        crit,crit_compare = torch.zeros(1).type(dtype),torch.zeros(1).type(dtype)\n",
    "        sol               = torch.zeros(size).type(dtype)\n",
    "        kappa             = torch.zeros(1).type(dtype)\n",
    "        xmin,xmax         = im_range\n",
    "        uTx               = torch.matmul(u,xtilde)\n",
    "        #set coefficients\n",
    "        a     = -(xmin+xmax+uTx)\n",
    "        b     = xmin*xmax + uTx*(xmin+xmax) - 2*gamma_mu*torch.norm(u)**2\n",
    "        c     = gamma_mu*(xmin+xmax) - uTx*xmin*xmax\n",
    "        p     = b - (a**2)/3\n",
    "        q     = c - a*b/3 + 2*(a**3)/27\n",
    "        delta = (p/3)**3 + (q/2)**2  \n",
    "\n",
    "        #three cases depending on the sign of delta\n",
    "        #########################################################################\n",
    "        #when delta is positive\n",
    "        if delta>0:\n",
    "            z1 = -q/2\n",
    "            z2 = torch.sqrt(delta)\n",
    "            u  = (z1+z2).sign() * torch.pow((z1+z2).abs(),1/3)\n",
    "            v  = (z1-z2).sign() * torch.pow((z1-z2).abs(),1/3) \n",
    "            x1 = u+v   \n",
    "            x2 = -(u + v)/2 ; #real part of the complex solution\n",
    "            x3 = -(u + v)/2 ; #real part of the complex solution\n",
    "        #########################################################################\n",
    "        #when delta is 0\n",
    "        elif delta==0:\n",
    "            x1 = 3 *q / p \n",
    "            x2 = -1.5 * q / p\n",
    "            x3 = -1.5 * q / p \n",
    "        #########################################################################\n",
    "        #when delta is negative\n",
    "        elif delta<0:\n",
    "            cos = (-q/2) * ((27 / torch.pow(p,3)).abs()).sqrt() \n",
    "            cos[cos<-1] = 0*cos[cos<-1]-1\n",
    "            cos[cos>1]  = 0*cos[cos>1]+1\n",
    "            phi         = torch.acos(cos)\n",
    "            tau         = 2 * ((p/3).abs()).sqrt() \n",
    "            x1     = tau * torch.cos(phi/3) \n",
    "            x2     = -tau * torch.cos((phi + np.pi)/3)\n",
    "            x3     = -tau * torch.cos((phi - np.pi)/3)\n",
    "        #########################################################################\n",
    "        x1   = x1-a/3\n",
    "        x2   = x2-a/3\n",
    "        x3   = x3-a/3\n",
    "        # when gamma_mu is very small there might be some numerical instabilities\n",
    "        # in case there are nan values, we set the corresponding pixels equal to 2*xmax\n",
    "        # these values will be replaced by valid values at least once\n",
    "        if (x1!=x1).any():\n",
    "            x1[x1!=x1]=2*xmax\n",
    "        if (x2!=x2).any():\n",
    "            x2[x2!=x2]=2*xmax\n",
    "        if (x3!=x3).any():\n",
    "            x3[x3!=x3]=2*xmax\n",
    "        kappa = x1\n",
    "        sol  = xtilde + (x1 - uTx)/torch.norm(u)**2*u\n",
    "        #########################################################################\n",
    "        #take x1\n",
    "        p1 = sol\n",
    "        uTp1 = torch.matmul(u,p1)\n",
    "        if (uTp1>xmin)&(uTp1<xmax):\n",
    "            crit[0] = -(torch.log(uTp1-xmin)+torch.log(xmax-uTp1))\n",
    "            crit = 0.5*torch.norm(p1-xtilde)**2+gamma_mu*crit\n",
    "        else:\n",
    "            crit[0] = np.inf\n",
    "        #########################################################################\n",
    "        #test x2\n",
    "        p2 = xtilde + (x2 - uTx)/torch.norm(u)**2*u\n",
    "        uTp2 = torch.matmul(u,p2)\n",
    "        if (uTp2 >xmin)&(uTp2 <xmax): \n",
    "            crit_compare[0]  = -(torch.log(uTp2-xmin)+torch.log(xmax-uTp2))\n",
    "            crit_compare  = 0.5*torch.norm(p2-xtilde)**2+gamma_mu*crit_compare\n",
    "        else:\n",
    "            crit_compare[0] = np.inf\n",
    "        if crit_compare<=crit:\n",
    "            kappa = x2\n",
    "            sol  = p2\n",
    "            crit = crit_compare\n",
    "        #########################################################################\n",
    "        #test x3\n",
    "        p3 = xtilde + (x3 - uTx)/torch.norm(u)**2*u\n",
    "        uTp3 = torch.matmul(u,p3)\n",
    "        if (uTp3>xmin)&(uTp3<xmax):\n",
    "            crit_compare[0] = -(torch.log(uTp3-xmin)+torch.log(xmax-uTp3))\n",
    "            crit_compare = 0.5*torch.norm(p3-xtilde)**2+gamma_mu*crit_compare\n",
    "        else:\n",
    "            crit_compare[0] = np.inf\n",
    "        if crit_compare<=crit:\n",
    "            kappa = x3\n",
    "            sol  = p3\n",
    "            crit = crit_compare\n",
    "        #########################################################################\n",
    "        # when gamma_mu is very small and xtilde is very close to one of the bounds,\n",
    "        # the solution of the cubic equation is not very well estimated -> test xtilde\n",
    "        # denom = (sol-xmin)*(sol-xmax)-2*gamma_mu -(sol-xtilde)*(xmin+xmax-2*sol)\n",
    "        if (uTx>xmin)&(uTx<xmax):\n",
    "            crit_compare = -(torch.log(xmax-uTx)+torch.log(uTx-xmin))\n",
    "            crit_compare = gamma_mu*crit_compare\n",
    "        else:\n",
    "            crit_compare[0] = np.inf\n",
    "        if crit_compare<crit :\n",
    "            kappa = uTx\n",
    "            sol = xtilde\n",
    "        \n",
    "        if mode_training==True:\n",
    "            ctx.save_for_backward(gamma_mu,xtilde,kappa)\n",
    "        return sol\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_var):\n",
    "        \"\"\"\n",
    "        Computes the first derivatives of the proximity operator of the log barrier with respect to x and gamma_mu.\n",
    "            This method is automatically called by the backward method of the loss function.\n",
    "        Parameters\n",
    "        ----------\n",
    "           ctx (list): list of torch.FloatTensors, variable saved during the forward operation\n",
    "           grad_output_var (torch.FloatTensor): gradient of the loss wrt the output of cardan\n",
    "        Returns\n",
    "        -------\n",
    "           grad_input_gamma_mu (torch.FloatTensor): gradient of the prox wrt gamma_m \n",
    "           grad_input_u (torch.FloatTensor): gradient of the prox wrt x\n",
    "           None: no gradient wrt u\n",
    "           None: no gradient wrt the image range\n",
    "           None: no gradient wrt the mode\n",
    "        \"\"\"\n",
    "        # Device CPU/GPU\n",
    "        device=\"cpu\"\n",
    "        if device == \"cuda\":\n",
    "            dtype = torch.cuda.FloatTensor\n",
    "        else :\n",
    "            dtype = torch.FloatTensor\n",
    "        #initialize variables\n",
    "        xmin           = 0\n",
    "        xmax           = 1\n",
    "        grad_output    = grad_output_var.data\n",
    "        gamma_mu, xtilde, kappa = ctx.saved_tensors\n",
    "        size           = xtilde.size()\n",
    "        u              = torch.ones(size).type(dtype)\n",
    "        uTx            = torch.matmul(u,xtilde)\n",
    "        Id             = torch.eye(size).type(dtype)\n",
    "        denom          = (kappa-xmin)*(kappa-xmax)\\\n",
    "                         -2*gamma_mu*torch.norm(u)**2\\\n",
    "                         -(kappa-uTx)*(xmin+xmax-2*kappa)\n",
    "        #\n",
    "        if denom.abs()<1e-7:\n",
    "            denom = denom +1\n",
    "            grad_input_gamma_mu = (2*kappa-(xmin+xmax))/denom*u\n",
    "            grad_input_x        = Id + ((kappa**2-kappa*(xmin+xmax)+xmin*xmax))/denom/torch.norm(u)**2*torch.matmul(u,u.T)\n",
    "            # if denom is very small, it means that gamma_mu is very small and sol is very close to one of the bounds,\n",
    "            # there is a discontinuity when gamma_mu tends to zero, if 0<sol<1 the derivative wrt x is approximately equal to \n",
    "            # 1 and the derivative wrt gamma_mu is approximated by 10^5 times the sign of 2*x[1-idx]-(xmin+xmax)\n",
    "            grad_input_gamma_mu = 0*grad_input_gamma_mu+1e5*torch.sign(2*kappa-(xmin+xmax))\n",
    "            grad_input_x        = 0*grad_input_x+1\n",
    "        else:\n",
    "            grad_input_gamma_mu = (2*kappa-(xmin+xmax))/denom*u\n",
    "            grad_input_x        = Id + ((kappa**2-kappa*(xmin+xmax)+xmin*xmax))/denom/torch.norm(u)**2*torch.matmul(u,u.T)\n",
    "        \n",
    "        \n",
    "        grad_input_gamma_mu = (grad_input_gamma_mu*grad_output).sum(1).sum(1).sum(1).unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "        grad_input_x        = grad_input_x*grad_output\n",
    "        \n",
    "        # safety check for numerical instabilities\n",
    "        if (grad_input_gamma_mu!=grad_input_gamma_mu).any():\n",
    "            print('there is a nan in grad_input_gamma_mu')\n",
    "        if (grad_input_x!=grad_input_x).any():\n",
    "            print('there is a nan in grad_input_u')\n",
    "            sys.exit()\n",
    "        \n",
    "        grad_input_gamma_mu = Variable(grad_input_gamma_mu.type(dtype),requires_grad=True)\n",
    "        grad_input_x        = Variable(grad_input_x.type(dtype),requires_grad=True)\n",
    "        \n",
    "        return grad_input_gamma_mu, grad_input_x, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2110)\n"
     ]
    }
   ],
   "source": [
    "x_tilde = torch.FloatTensor([0.01,2,3.8,23,9,0,2])\n",
    "u = torch.FloatTensor(0.01*np.linspace(0,1,7))\n",
    "print(torch.matmul(u,x_tilde))\n",
    "im_range = [0,1]\n",
    "gamma = 1\n",
    "mu = 0.1\n",
    "\n",
    "out = cardan.apply(gamma*mu,x_tilde,u,im_range,\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-02, 2.0000e+00, 3.8000e+00, 2.3000e+01, 9.0000e+00, 0.0000e+00,\n",
       "        2.0000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-617965056ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
